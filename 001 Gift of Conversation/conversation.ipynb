{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3cafa0-2b58-4cf9-90ec-c93e078451ab",
   "metadata": {},
   "source": [
    "# The Gift of Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261fc078-abd0-40f7-849c-91a7f533fdd4",
   "metadata": {},
   "source": [
    "This is the accompanying notebook for my [blog post](https://cannect.canyalniz.com/001-gift-of-conversation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309020f8-464c-48b8-be45-0fbe8dfff3a8",
   "metadata": {},
   "source": [
    "My girlfriend Çağla is a die hard fan of the movie Legally Blonde and an avid admirer of the movie's heroine Elle Woods. Elle's indomitable spirit, loyalty to her friends and high held moral values are among the reasons Çağla cherishes her so much. So, I thought I would use my programming skills and the capabilities of today's artificial intelligence models to facilitate a meeting between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f89220-da3d-4ed3-9639-038059e9ac4a",
   "metadata": {},
   "source": [
    "## Setting API Keys with Environment Variables\n",
    "In order to authenticate ourselves during the API calls we'll need to provide our program with our keys. The LangChain OpenAI API module expects to find the API key in its dedicated environment variable while the SpeechRecognition Whisper API module and the ElevenLabs API module expect their respective keys as function arguments. To suit all needs and prevent possible leakage of the keys, I recommend setting the environment variables for both keys using your preferred method. You can load the keys to variables within the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1b20a-b1e0-44cc-af36-3e5fff02c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ecf8c-2a97-4645-9281-9e5678459384",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") # Your OpenAI API Key goes here\n",
    "ELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\") # Your Eleven Labs API Key goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd02575-20fc-4128-9fd5-0ece84031362",
   "metadata": {},
   "source": [
    "## Voice Input\n",
    "We'll use the [SpeechRecognition](https://github.com/Uberi/speech_recognition#readme) library for capturing the user's voice and converting it to text. Every turn the program will start recording audio when the user starts speaking and record until they stop speaking. Once the recording is done the audio file will be sent to Whisper to be converted into text.\n",
    "\n",
    "We'll start by initializing our recognizer object which we will use to both capture the audio and make the API call to Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bd5c1-59b1-4aa4-a8b9-7f5c92a70ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1bf3b3-1138-4d8a-b399-ef5a546b82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain audio from the microphone\n",
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b7ab0-392a-496a-931f-ca484f4f5195",
   "metadata": {},
   "source": [
    "Our `Recognizer` object uses its `energy_threshold` to detect speech over normal background noise so that it can start recording. Let's configure it automatically for our environment. The below code should work well for the majority, however if you have issues with the energy threshold after automatic calibration check out the sectionin the appendix for manual configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9101e76-69ed-433b-bcd7-4b48438c2f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sr.Microphone() as source: # use the default microphone as the audio source\n",
    "    r.adjust_for_ambient_noise(source, duration=5) # listen for 5 seconds to calibrate the energy threshold for ambient noise levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c2afdb-f61d-44e5-a455-4d2e9454246a",
   "metadata": {},
   "source": [
    "Once the energy threshold is set to the appropriate value, the `Recognizer` can start recording when it detects speech. Let's listen for some input and store the recording in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7534c-1fe2-4f6f-8933-cc0a86a962b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sr.Microphone() as source:\n",
    "    audio = r.listen(source)\n",
    "    print(\"Captured next line...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669ceb2-3d1d-40ed-82a9-208b8b61fe9f",
   "metadata": {},
   "source": [
    "## The Heart, Soul and Brain of the Character\n",
    "Now that we have the user's input, we will process it and generate a response as our character within the given context, while also paying attention to the history of the conversation. We'll achieve this using the `ConversationChain` class from the `LangChain` Python library. `ConversationClass` will abstract away a lot of details for us to be able to focus on building the character and the story without worrying about the lower level details of prompting the LLM iteratively or manually handling the history of the conversation.\n",
    "\n",
    "### the LLM\n",
    "We use the `ChatOpenAI` module offered by LangChain to initialize our llm. We need to take care of two things when setting up our OpenAI llm what model to use and the temperature of the model. For this application, among the currently available models, `gpt-3.5-turbo` is the tool for the job. It has the dialogue operation capabilities we're looking for and it is very reasonably priced. As of the writing of this post `gpt-3.5-turbo` is the default model `ChatOpenAI` uses. As for the temperature, setting it to 0.7 yielded good results for my use case, feel free to try out different values yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca129e3-d3a0-49c3-987e-784e68ee056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ac9d0-d039-47b6-bd89-034c8134ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f340f9-d52c-41cf-9f74-f03498bd2137",
   "metadata": {},
   "source": [
    "### Custom Prompt Template for the ConversationChain\n",
    "With the core of the prompt ready, we will create a custom prompt template with which our ConversationChain object will be able to drive the conversation. We want our prompt template to accept two input variables: `history` to keep track of the conversation so far and `input` to inject the user's input into the prompt. We position the input variables in curly braces inside our prompt template, and set the template up to end on `Elle:` to have our llm generate the next line as Elle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687172e-2f10-4a3a-b0ef-3a2be4919642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf100b1-cc20-4fa5-aecd-3ef003a5aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_prompt_template = PromptTemplate(\n",
    "    input_variables=['history', 'input'],\n",
    "    output_parser=None,\n",
    "    partial_variables={},\n",
    "    template=\"\"\"\\\n",
    "    The following is a friendly phone call between the character Elle Woods from the movie Legally Blonde and her best friend Chaala.\\\n",
    "    Elle is compassionate, caring, supportive, talkative and empathetic.\\\n",
    "    Chaala is on a journey to find herself, Elle will support her and encourage Chaala to believe in herself.\\\n",
    "    Today is Chaala's birthday.\\\n",
    "    Elle wants to learn more about Chaala's life and catch-up.\\\n",
    "    Elle pays attention to details from the context of the conversation and accurately represents her character from Legally Blonde.\\\n",
    "    \n",
    "\n",
    "    \n",
    "    Current conversation:\n",
    "    {history}\n",
    "    Chaala: {input}\n",
    "    Elle:\"\"\",\n",
    "    template_format='f-string',\n",
    "    validate_template=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed8632-b1fa-4764-b5f4-aafcf7991c1b",
   "metadata": {},
   "source": [
    "### Conversation Memory\n",
    "To generate the response and drive a meaningful conversation, the llm must have an idea of what has been talked about up to the current line. We can convey this information to our llm using one of the [many memory modules offered by LangChain](https://python.langchain.com/docs/modules/memory/types/). Our most notable options are:\n",
    "\n",
    "- [ConversationBufferWindowMemory](https://python.langchain.com/docs/modules/memory/types/buffer): Retains the last `k` turns of the conversation verbatim, where `k` is the window size\n",
    "- [ConversationSummaryMemory](https://python.langchain.com/docs/modules/memory/types/summary): At each turn performs a call to the llm using a custom prompt to keep a running summary of the conversation history\n",
    "- [ConversationSummaryBufferMemory](https://python.langchain.com/docs/modules/memory/types/summary_buffer): Keeps the most recent turns of the conversation verbatim and progressively summarizes older lines that fall above a token limit\n",
    "\n",
    "We will be using the `ConversationSummaryBufferMemory` module which in my opinion strikes a good balance regarding information retention without getting too complicated. To utilize the summary feature offered by this module we need a slight modification. The default progressive summarization prompt refers to the user as `the human` and the character as `the AI`. Let's create a custom PromptTemplate by referencing the participants appropriately, the user as `Chaala` and the character as `Elle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3983413f-f067-468d-bfa2-11162488be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_prompt_template = PromptTemplate(\n",
    "    input_variables=['summary', 'new_lines'],\n",
    "    output_parser=None,\n",
    "    partial_variables={},\n",
    "    template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nChaala asks what Elle thinks of artificial intelligence. Elle thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nChaala Why do you think artificial intelligence is a force for good?\\nElle: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nChaala asks what Elle thinks of artificial intelligence. Elle thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:',\n",
    "    template_format='f-string',\n",
    "    validate_template=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b5fbb-00d7-4ac6-80c8-c40c598a1023",
   "metadata": {},
   "source": [
    "Let's set the maximum token limit above which summarization will occur to be 350 tokens and initialize our memory object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b6d56-1485-41b7-a105-0132487a7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729d5d9-c2f9-4db0-847a-972c990411dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=350,\n",
    "    prompt=summarizer_prompt_template,\n",
    "    ai_prefix=\"Elle\",\n",
    "    human_prefix=\"Chaala\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d326b-8f8f-4745-9389-916510aa7b65",
   "metadata": {},
   "source": [
    "### Initializing the ConversationChain\n",
    "We can now use the parts we have built to create our ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7aaf9-c630-45c3-a0d4-b2e6539efbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc11218-adf2-404e-89cc-64fb02f5f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt = conversation_prompt_template,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8ff4a-6581-4342-a97d-795fb09819e2",
   "metadata": {},
   "source": [
    "## Giving Your Character a Voice\n",
    "\n",
    "### Voice Cloning\n",
    "You can check out [ElevenLabs](https://elevenlabs.io/) for a speech synthesis interface which you can use to capture the voice of the character you have chosen. You are going to need some high quality clips of your character speaking for the best results.\n",
    "\n",
    "### Speech Generation\n",
    "Once we have the voice designed to our liking, we can make the API call to have it read for us. To start we need to create identifiers for the voice we have created and the model we wish to use for generation. I was given access to the Eleven English v2 model from ElevenLabs upon request. Although this model is in Beta as of the writing of this post it produced better results for me. Make sure to refer to your voice with the name you gave it on the ElevenLabs platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7032c0-9088-4b52-9c27-68017fa3591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import set_api_key\n",
    "from elevenlabs.api import Models\n",
    "from elevenlabs.api import Voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79b593-0c0f-4486-849c-16ba1fe241d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_api_key(ELEVENLABS_API_KEY)\n",
    "\n",
    "models = Models.from_api()\n",
    "elle_model = [model for model in models if model.name == \"Eleven English v2\"][0]\n",
    "\n",
    "voices = Voices.from_api()\n",
    "elle_voice = [voice for voice in voices if voice.name == \"Elle Woods\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5de247-4867-4b62-8511-76ffc6130a77",
   "metadata": {},
   "source": [
    "## The Conversation Loop\n",
    "We are now ready to put everything together. Since our conversation will consist of the user and the character taking turns to speak, we'll wrap the whole thing in a loop and have it run until interruption. We can also wrap the loop with a try-except block to pickle the conversation memory before exiting the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb276d-1511-4e0f-af9a-1e68f9b13cfd",
   "metadata": {},
   "source": [
    "### Prelude\n",
    "Since this is likely to be a surprise birthday gift, let's include an introduction to the conversation. For my gift I had one of the default voices of ElevenLabs read the following text.\n",
    "```markdown\n",
    "Initiating contact with Elle Woods. This is an inter-dimensional phone call. Voice delays and awkward pauses are expected. Connecting now...\n",
    "```\n",
    "And followed this introduction with a [phone line sound effect](https://www.youtube.com/watch?v=nHRVQKY1xh4) that eventually gets picked up. Upon picking up the phone a pre-generated audio of the character saying `Hello Chaala!` is played. I chose to play the files using mpv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b26957-e32d-4539-bfa5-cde1d85ff6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mpv initiating-2.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4058750-17bc-460e-bbfa-4f99834109e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mpv --volume=65 ring.opus\")\n",
    "os.system(\"mpv elle-greeting.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0228a4-eecc-47cb-b78d-8a58f3c32fe7",
   "metadata": {},
   "source": [
    "### The Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fec29-d3ce-49cb-8e91-ec1e3ffb38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        # capture spoken user input\n",
    "        with sr.Microphone() as source:\n",
    "            audio = r.listen(source)\n",
    "            print(\"Captured next line...\")\n",
    "\n",
    "        # convert the user input into text\n",
    "        input_text = r.recognize_whisper_api(audio, api_key=OPENAI_API_KEY)\n",
    "        \n",
    "        # generate text response to the user input\n",
    "        response_text = conversation.predict(input=input_text)\n",
    "\n",
    "        # generate and stream the character's voice\n",
    "        audio_stream = generate(\n",
    "            text=response_text,\n",
    "            voice=elle_voice,\n",
    "            model=elle_model,\n",
    "            stream=True\n",
    "        )\n",
    "        stream(audio_stream)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    # play disconnection effect\n",
    "    os.system(\"mpv end.opus\")\n",
    "    # save the conversation memory to disk\n",
    "    with open(\"conversation_memory\", \"wb\") as f:\n",
    "        pickle.dump(conversation.memory, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e142b6a-66ab-41b4-a444-ad76f2e5fba9",
   "metadata": {},
   "source": [
    "With all our components working together, this chain of artificial intelligence models will imitate a conversation with the chosen character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03bddb-02c4-49b0-9413-3b67fbb49569",
   "metadata": {},
   "source": [
    "## Stress Points\n",
    "As you will see upon testing it out yourself, for the most part this system works pretty well. **SpeechRecognition** knows where to start and stop the recording, **Whisper** is quite successful in discerning what is said, **ChatGPT** generates theme-appropriate responses, and **ElevenLabs English v2** does a good job of generating convincing speech. However there are points in the system that require special attention and one caveat I haven't been able to mitigate.\n",
    "\n",
    "- **SpeechRecognition Configuration**: Timing the recording successfully requires the correct configuration of the `Energy Threshold` and `Pause Threshold` parameters. If the automatic configuration isn't working out for you check out the [manual configuration guides in the appendix](#manual-configuration-of-the-speechrecognition-energy-threshold).\n",
    "- **ElevenLabs Voice Lab**: Getting the voice design right requires high quality of the character speaking as well as a certain amount of experimentation.\n",
    "- **Response Latency**: Unfortunately there is one aspect of the use experience I haven't been able to fix yet: the latency of the spoken response. The biggest culprit here is the ElevenLabs API call. Even with streaming enabled, the speech generation takes a long time to complete. Paired with the `PauseThreshold` amount of seconds `SpeechRecognition` waits before concluding its recording, you can expect to experience anywhere from 10 to 20 seconds of total response latency. Although this is quite high for normal conversations, the *inter-dimensional* nature of the conversation made it acceptable in my experience. If you have powerful enough hardware you can attempt to get around this problem by opting to run a tts model locally (such as [tortoise-tts](https://github.com/neonbjb/tortoise-tts))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45372c54-3cf1-491a-a3a6-7767ffe2402f",
   "metadata": {},
   "source": [
    "## Moderating the Conversation\n",
    "You are now ready to give the gift of inter-dimensional conversation. Let me tell you how I moderated the call. To preserve the mystery around what was about to happen I had Çağla sit across from me so that she couldn't see the screen. Gave her the headphones and went through the Manual Configuration of the SpeechRecognition Energy Threshold. Once I was happy with the configuration, I executed Prelude together with Conversation Loop so that recording started as soon as Elle greeted Çağla. When the conversation was over, I interrupted the execution of the program to start the Epilogue and save the conversation memory to disk. I also recommend setting up a camera to record a video of the conversation, the reaction I got made the effort of putting this together well worth it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conv-AI",
   "language": "python",
   "name": "conv-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
